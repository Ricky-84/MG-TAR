{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242f8535-db1e-4a3b-82aa-9f50e0147bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, logging, itertools, json\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import data_loader\n",
    "from evaluator import compute_error, stepwise_error\n",
    "from tqdm.notebook import tqdm\n",
    "from MG_TAR import MG_TAR\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47684d83-4b2d-4972-a2da-383dc803b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69862dc0-4a05-4002-aaa2-532b34c29456",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Seoul'\n",
    "year = '2016' # '2016' or '2018'\n",
    "n_steps, length = 6, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97ea742-85fb-45c9-bd1f-c8a2a3cfef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'jaccard'\n",
    "\n",
    "with open('./model_configs.json') as f:\n",
    "    model_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f358142f-4f44-447c-8e80-08722e064c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Seoul (2016):\n",
      "MSE: 2.609439169474812 - Acc@K: 0.39492753623188404 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = data_loader('./datasets', city, year, length=length, n_steps=n_steps, is_scale=True, temporal_copy=True)\n",
    "n_districts = len(datasets['selected_areas'])\n",
    "\n",
    "# Extract Features to their corresponding variables\n",
    "risk_train, risk_test = datasets['risk'][0], datasets['risk'][1]\n",
    "demo_train, demo_test = datasets['demo'][0], datasets['demo'][1]\n",
    "poi_train, poi_test = datasets['poi'][0], datasets['poi'][1]\n",
    "road_train, road_test = datasets['road'][0], datasets['road'][1]\n",
    "volume_train, volume_test = datasets['volume'][0], datasets['volume'][1]\n",
    "speed_train, speed_test = datasets['speed'][0], datasets['speed'][1]\n",
    "weather_train, weather_test = datasets['weather'][0], datasets['weather'][1]\n",
    "calendar_train, calendar_test = datasets['calendar'][0], datasets['calendar'][1]\n",
    "c_train, c_test = datasets['dtg'][0], datasets['dtg'][1]\n",
    "y_train, y_test = datasets['y'][0], datasets['y'][1]\n",
    "\n",
    "\n",
    "# Train - Validation Split\n",
    "val_idx = round(risk_train.shape[0] * 0.10) # 10% of Train Set\n",
    "risk_train, risk_val = risk_train[:-val_idx], risk_train[-val_idx:]\n",
    "demo_train, demo_val = demo_train[:-val_idx], demo_train[-val_idx:]\n",
    "poi_train, poi_val = poi_train[:-val_idx], poi_train[-val_idx:]\n",
    "road_train, road_val = road_train[:-val_idx], road_train[-val_idx:]\n",
    "volume_train, volume_val = volume_train[:-val_idx], volume_train[-val_idx:]\n",
    "speed_train, speed_val = speed_train[:-val_idx], speed_train[-val_idx:]\n",
    "weather_train, weather_val = weather_train[:-val_idx], weather_train[-val_idx:]\n",
    "calendar_train, calendar_val = calendar_train[:-val_idx], calendar_train[-val_idx:]\n",
    "c_train, c_val = c_train[:-val_idx], c_train[-val_idx:]\n",
    "y_train, y_val = y_train[:-val_idx], y_train[-val_idx:]\n",
    "\n",
    "# Contextual & Adjacency Matrices\n",
    "A = pd.read_csv(f'./datasets/graph_data/{city}-normalized-district.csv', engine='c', index_col=0).to_numpy()\n",
    "A_poi = pd.read_csv(f'./datasets/graph_data/{city}-{year}-poi-normalized-district-{metric}.csv', engine='c', index_col=0).to_numpy()\n",
    "A_demo = pd.read_csv(f'./datasets/graph_data/{city}-{year}-demo-normalized-district-{metric}.csv', engine='c', index_col=0).to_numpy()\n",
    "A_road = pd.read_csv(f'./datasets/graph_data/{city}-{year}-road-normalized-district-{metric}.csv', engine='c', index_col=0).to_numpy()\n",
    "\n",
    "A_train, A_val = np.tile(A, (risk_train.shape[0], 1, 1)), np.tile(A, (risk_val.shape[0], 1, 1))\n",
    "A_poi_train, A_poi_val = np.tile(A_poi, (risk_train.shape[0], 1, 1)), np.tile(A_poi, (risk_val.shape[0], 1, 1))\n",
    "A_demo_train, A_demo_val = np.tile(A_demo, (risk_train.shape[0], 1, 1)), np.tile(A_demo, (risk_val.shape[0], 1, 1))\n",
    "A_road_train, A_road_val = np.tile(A_road, (risk_train.shape[0], 1, 1)), np.tile(A_road, (risk_val.shape[0], 1, 1))\n",
    "\n",
    "A_test = np.tile(A, (risk_test.shape[0], 1, 1))\n",
    "A_poi_test = np.tile(A_poi, (risk_test.shape[0], 1, 1))\n",
    "A_demo_test = np.tile(A_demo, (risk_test.shape[0], 1, 1))\n",
    "A_road_test = np.tile(A_road, (risk_test.shape[0], 1, 1))\n",
    "\n",
    "with open(f'datasets/graph_data/{city}-{year}-traffic-district-normalized-train-{metric}.npy', 'rb') as f:\n",
    "    A_traffic_train = np.load(f)\n",
    "    A_traffic_train, A_traffic_val = A_traffic_train[:-val_idx], A_traffic_train[-val_idx:]\n",
    "\n",
    "with open(f'datasets/graph_data/{city}-{year}-traffic-district-normalized-test-{metric}.npy', 'rb') as f:\n",
    "    A_traffic_test = np.load(f)\n",
    "\n",
    "# Features Aggregation --> inputs=[A, A_poi, A_demo, A_road, A_traffic, X]\n",
    "node_features_train = np.concatenate([risk_train, demo_train, poi_train, road_train, volume_train, speed_train, weather_train, calendar_train, c_train], axis=-1)\n",
    "node_features_val = np.concatenate([risk_val, demo_val, poi_val, road_val, volume_val, speed_val, weather_val, calendar_val, c_val], axis=-1)\n",
    "node_features_test = np.concatenate([risk_test, demo_test, poi_test, road_test, volume_test, speed_test, weather_test, calendar_test, c_test], axis=-1)\n",
    "\n",
    "x_train = [A_train, A_poi_train, A_demo_train, A_road_train, A_traffic_train, node_features_train]\n",
    "x_val = [A_val, A_poi_val, A_demo_val, A_road_val, A_traffic_val, node_features_val]\n",
    "x_test = [A_test, A_poi_test, A_demo_test, A_road_test, A_traffic_test, node_features_test]\n",
    "\n",
    "# Load Saved Model\n",
    "best_config = model_configs[year][0]\n",
    "best_model = MG_TAR(x_train, y_train, x_val, y_val, best_config)\n",
    "\n",
    "y_pred = best_model.predict(x_test)\n",
    "sw_errors = stepwise_error(y_test, y_pred, n_steps)\n",
    "\n",
    "print(f'Model Performance Seoul ({year}):')\n",
    "print(f\"MSE: {np.average(sw_errors['MSE'])} - Acc@K: {np.average(sw_errors['ACC'])} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c7a22-6084-4588-9f60-f0bbbab75b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (patara)",
   "language": "python",
   "name": "patara"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
